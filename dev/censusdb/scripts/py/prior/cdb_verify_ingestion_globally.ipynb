{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0701a936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import duckdb\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beefcab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bucket_stats(bucket_dir: Path) -> tuple[int, float, list[Path]]:\n",
    "    files = sorted(bucket_dir.glob(\"bucket_*.parquet\"))\n",
    "    total_rows = 0\n",
    "    total_size_mb = 0.0\n",
    "    missing = []\n",
    "    for f in files:\n",
    "        if not f.exists():\n",
    "            missing.append(f)\n",
    "            continue\n",
    "        try:\n",
    "            df = pl.read_parquet(f)\n",
    "            total_rows += df.shape[0]\n",
    "            total_size_mb += f.stat().st_size / (1024 * 1024)\n",
    "        except Exception:\n",
    "            missing.append(f)\n",
    "    return total_rows, round(total_size_mb, 2), missing\n",
    "\n",
    "def get_manifest_stats(manifest_db: Path, cenyear: int) -> tuple[int, list[str]]:\n",
    "    with duckdb.connect(str(manifest_db)) as con:\n",
    "        df = pl.from_arrow(con.execute(\n",
    "            \"SELECT file, record_count FROM manifest WHERE cenyear = ?\", [cenyear]\n",
    "        ).arrow())\n",
    "    total = df[\"record_count\"].sum()\n",
    "    files = df[\"file\"].to_list()\n",
    "    return total, files\n",
    "\n",
    "def get_index_stats(index_db: Path, cenyear: int) -> int:\n",
    "    year_str = str(cenyear)\n",
    "    with duckdb.connect(str(index_db)) as con:\n",
    "        return con.execute(\"SELECT COUNT(*) FROM index WHERE hhid LIKE ?\", [f\"{year_str}%\"]).fetchone()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb8bb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_all_years(basepath: Path, folder_map: dict[str, Path], output_csv: Path):\n",
    "    manifest_db = basepath / \"manifests\" / \"manifest.duckdb\"\n",
    "    index_db = basepath / \"index\" / \"search_index.duckdb\"\n",
    "\n",
    "    summary_rows = []\n",
    "    for year_str, bucket_dir in folder_map.items():\n",
    "        cenyear = int(year_str)\n",
    "        bucket_rows, bucket_size, missing_files = get_bucket_stats(bucket_dir)\n",
    "        manifest_rows, manifest_files = get_manifest_stats(manifest_db, cenyear)\n",
    "        index_rows = get_index_stats(index_db, cenyear)\n",
    "\n",
    "        expected_bucket_count = len(manifest_files)\n",
    "        actual_bucket_count = len(bucket_dir.glob(\"bucket_*.parquet\"))\n",
    "\n",
    "        problems = []\n",
    "        if len(missing_files) > 0:\n",
    "            problems.append(\"MissingFiles\")\n",
    "        if actual_bucket_count != expected_bucket_count:\n",
    "            problems.append(\"MissingBuckets\")\n",
    "        if bucket_rows != manifest_rows:\n",
    "            problems.append(\"RowMismatch\")\n",
    "        if index_rows == 0:\n",
    "            problems.append(\"IndexEmpty\")\n",
    "        elif index_rows < manifest_rows * 0.5:\n",
    "            problems.append(\"IndexMismatch\")\n",
    "\n",
    "        summary_rows.append({\n",
    "            \"cenyear\": cenyear,\n",
    "            \"bucket_rows\": bucket_rows,\n",
    "            \"bucket_size_mb\": bucket_size,\n",
    "            \"manifest_rows\": manifest_rows,\n",
    "            \"index_rows\": index_rows,\n",
    "            \"missing_files\": len(missing_files),\n",
    "            \"status\": \"OK\" if not problems else \",\".join(problems)\n",
    "        })\n",
    "\n",
    "    df_summary = pl.DataFrame(summary_rows)\n",
    "\n",
    "    # Print tabulated summary\n",
    "    print(\"\\nðŸ“Š Summary Table:\")\n",
    "    print(tabulate(df_summary.to_dicts(), headers=\"keys\", tablefmt=\"github\"))\n",
    "\n",
    "    # Save to CSV\n",
    "    df_summary.write_csv(output_csv)\n",
    "    print(f\"\\nâœ… Summary written to {output_csv}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4677ca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_locations_table(basepath: Path):\n",
    "    loc_folder = basepath / \"locations\"\n",
    "    loc_csv = loc_folder / \"cloc.csv\"\n",
    "    loc_db = loc_folder / \"locations.duckdb\"\n",
    "\n",
    "    df = pl.read_csv(loc_csv).with_row_count(name=\"locid\")\n",
    "    df = df.select([\"cloc\", \"stateicp\", \"countyicp\", \"statename\", \"countyname\", \"locid\"])\n",
    "\n",
    "    loc_folder.mkdir(exist_ok=True)\n",
    "    with duckdb.connect(str(loc_db)) as con:\n",
    "        con.execute(\"CREATE OR REPLACE TABLE locations AS SELECT * FROM df\")\n",
    "\n",
    "    print(f\"âœ… Created locations.duckdb with {df.shape[0]} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def verify_cloc_values(basepath: Path, folder_map: dict[str, Path]):\n",
    "    loc_db = basepath / \"locations\" / \"locations.duckdb\"\n",
    "    manifest_db = basepath / \"manifests\" / \"manifest.duckdb\"\n",
    "    index_db = basepath / \"index\" / \"search_index.duckdb\"\n",
    "\n",
    "    with duckdb.connect(str(loc_db)) as con:\n",
    "        cloc_known = set(con.execute(\"SELECT cloc FROM locations\").fetchall())\n",
    "\n",
    "    cloc_used = set()\n",
    "\n",
    "    # From manifest\n",
    "    with duckdb.connect(str(manifest_db)) as con:\n",
    "        rows = con.execute(\"\"\"\n",
    "            SELECT DISTINCT min_locid AS cloc FROM manifest\n",
    "            UNION\n",
    "            SELECT DISTINCT max_locid AS cloc FROM manifest\n",
    "        \"\"\").fetchall()\n",
    "        cloc_used.update([r[0] for r in rows if r[0] is not None])\n",
    "\n",
    "    # From index\n",
    "    with duckdb.connect(str(index_db)) as con:\n",
    "        rows = con.execute(\"SELECT DISTINCT cloc FROM index\").fetchall()\n",
    "        cloc_used.update([r[0] for r in rows if r[0] is not None])\n",
    "\n",
    "    # From buckets\n",
    "    for year_str, bucket_dir in folder_map.items():\n",
    "        for f in bucket_dir.glob(\"bucket_*.parquet\"):\n",
    "            try:\n",
    "                df = pl.read_parquet(f)\n",
    "                if \"cloc\" in df.columns:\n",
    "                    cloc_used.update(df[\"cloc\"].unique().to_list())\n",
    "            except Exception:\n",
    "                print(f\"âš ï¸ Failed to read {f}\")\n",
    "\n",
    "    missing = sorted(cloc_used - cloc_known)\n",
    "    if missing:\n",
    "        print(f\"\\nâš ï¸ {len(missing)} cloc values used in data but missing from locations table:\")\n",
    "        print(tabulate([{\"cloc\": c} for c in missing], headers=\"keys\", tablefmt=\"github\"))\n",
    "    else:\n",
    "        print(\"âœ… All cloc values used in the database are present in the locations table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77581ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_cloc_values(basepath: Path, folder_map: dict[str, Path]):\n",
    "    missing_files = []\n",
    "\n",
    "    for year_str, bucket_dir in folder_map.items():\n",
    "        for f in bucket_dir.glob(\"bucket_*.parquet\"):\n",
    "            try:\n",
    "                df = pl.read_parquet(f, n_rows=1)\n",
    "                if \"cloc\" not in df.columns:\n",
    "                    missing_files.append({\"year\": year_str, \"file\": f.name})\n",
    "            except Exception:\n",
    "                missing_files.append({\"year\": year_str, \"file\": f.name + \" (unreadable)\"})\n",
    "\n",
    "    if missing_files:\n",
    "        print(f\"\\nâš ï¸ {len(missing_files)} bucket files missing 'cloc' column:\")\n",
    "        print(tabulate(missing_files, headers=\"keys\", tablefmt=\"github\"))\n",
    "    else:\n",
    "        print(\"âœ… All bucket files contain a 'cloc' column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf2e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_index_indexes(basepath: Path):\n",
    "    index_db = basepath / \"index\" / \"search_index.duckdb\"\n",
    "    with duckdb.connect(str(index_db)) as con:\n",
    "        con.execute(\"CREATE INDEX IF NOT EXISTS idx_hhid ON index(hhid);\")\n",
    "        con.execute(\"CREATE INDEX IF NOT EXISTS idx_cloc ON index(cloc);\")\n",
    "    print(\"âœ… Created indexes on search_index: hik (unique), hhid (filterable), cloc (filterable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6955ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_manifest_indexes(basepath: Path):\n",
    "    manifest_db = basepath / \"manifests\" / \"manifest.duckdb\"\n",
    "    with duckdb.connect(str(manifest_db)) as con:\n",
    "        # Index for fast filtering by year\n",
    "        con.execute(\"CREATE INDEX IF NOT EXISTS idx_manifest_cenyear ON manifest(cenyear);\")\n",
    "\n",
    "        # Index for range-based hhid filtering (used in BETWEEN queries)\n",
    "        con.execute(\"CREATE INDEX IF NOT EXISTS idx_manifest_min_hhid ON manifest(min_hhid);\")\n",
    "        con.execute(\"CREATE INDEX IF NOT EXISTS idx_manifest_max_hhid ON manifest(max_hhid);\")\n",
    "\n",
    "        # Optional: compound index for year + hhid range filtering\n",
    "        con.execute(\"CREATE INDEX IF NOT EXISTS idx_manifest_year_hhid ON manifest(cenyear, min_hhid, max_hhid);\")\n",
    "\n",
    "    print(\"âœ… Created indexes on manifest: cenyear, min_hhid, max_hhid, and compound year+hhid\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab49564d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Note: Notes on Manifest Indexing\n",
    "- DuckDB does not support range indexes directly, so indexing both min_hhid and max_hhid allows efficient filtering using hhid BETWEEN min_hhid AND max_hhid.\n",
    "- You can later extract records for a list of hhid values using a join or WHERE EXISTS.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f1219e7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "if __name__ == \"__main__\":\n",
    "    BASEPATH = Path(\"D:/censusdb\")\n",
    "    folder_map = {\n",
    "        \"1850\": BASEPATH / \"cp1850\",\n",
    "        \"1860\": BASEPATH / \"cp1860\",\n",
    "        \"1870\": BASEPATH / \"cp1870\",\n",
    "        \"1880\": BASEPATH / \"cp1880\",\n",
    "        \"1890\": BASEPATH / \"cp1890\",\n",
    "        \"1900\": BASEPATH / \"cp1900\",\n",
    "        \"1910\": BASEPATH / \"cp1910\",\n",
    "        \"1920\": BASEPATH / \"cp1920\",\n",
    "    }\n",
    "    summarize_all_years(BASEPATH, folder_map, BASEPATH / \"summary_all_years.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
