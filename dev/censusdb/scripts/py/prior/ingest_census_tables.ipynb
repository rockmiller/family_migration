{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c803245",
   "metadata": {},
   "source": [
    "Census Ingestion Script<br>\n",
    "<small> Final Version with DuckDB Manifest and Search Index </small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e6b2673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import hashlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "from tabulate import tabulate\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9001fbae",
   "metadata": {},
   "source": [
    "Define Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83414d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASEPATH = Path(\"D:/censusdb\")\n",
    "SOURCEPATH = Path(\"D:/source\")\n",
    "\n",
    "folder_map = {\n",
    "    \"1850\": BASEPATH / \"cp1850\",\n",
    "    \"1860\": BASEPATH / \"cp1860\",\n",
    "    \"1870\": BASEPATH / \"cp1870\",\n",
    "    \"1880\": BASEPATH / \"cp1880\",\n",
    "    \"1890\": BASEPATH / \"cp1890\",\n",
    "    \"1900\": BASEPATH / \"cp1900\",\n",
    "    \"1910\": BASEPATH / \"cp1910\",\n",
    "    \"1920\": BASEPATH / \"cp1920\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "553a6c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_for_year(cenyear: int, out_root: Path) -> dict:\n",
    "    cenyear_str = str(cenyear)\n",
    "    csv_root = SOURCEPATH\n",
    "    csv_template = \"cs{cenyear}.csv.gz\"\n",
    "    index_db = BASEPATH / \"index\" / \"search_index.duckdb\"\n",
    "    manifest_db = BASEPATH / \"manifests\" / \"manifest.duckdb\"\n",
    "\n",
    "    buckets_map = {\n",
    "        \"1850\": 8, \"1860\": 12, \"1870\": 16, \"1880\": 16,\n",
    "        \"1890\": 12, \"1900\": 16, \"1910\": 20, \"1920\": 24\n",
    "    }\n",
    "\n",
    "    if cenyear_str not in buckets_map:\n",
    "        raise ValueError(f\"No bucket count defined for year {cenyear}\")\n",
    "\n",
    "    csv_path = csv_root / csv_template.format(cenyear=cenyear_str)\n",
    "    bucket_count = buckets_map[cenyear_str]\n",
    "\n",
    "    return {\n",
    "        \"cenyear\": cenyear,\n",
    "        \"csv\": csv_path,\n",
    "        \"bucket_count\": bucket_count,\n",
    "        \"out_root\": out_root,\n",
    "        \"index_db\": index_db,\n",
    "        \"manifest_db\": manifest_db,\n",
    "        \"state_col\": \"stateicp\",\n",
    "        \"county_col\": \"countyicp\",\n",
    "        \"serial_col\": \"serial\",\n",
    "        \"histid_col\": \"histid\",\n",
    "        \"hik_col\": \"hik\",\n",
    "        \"read_csv_opts\": {\n",
    "            \"has_header\": True,\n",
    "            \"separator\": \",\",\n",
    "            \"infer_schema_length\": 10000\n",
    "        }\n",
    "    }\n",
    "\n",
    "def config_for_filename(filename: str, folder_map: dict[str, Path]) -> dict:\n",
    "    stem = Path(filename).stem  # 'cs1860'\n",
    "    cenyear_str = stem[2:6]     # '1860'\n",
    "\n",
    "    if cenyear_str not in folder_map:\n",
    "        raise ValueError(f\"No output folder defined for cenyear {cenyear_str}\")\n",
    "\n",
    "    cenyear = int(cenyear_str)\n",
    "    out_root = folder_map[cenyear_str]\n",
    "    return config_for_year(cenyear, out_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e3b8fd",
   "metadata": {},
   "source": [
    "Ingestion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31bd16ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_output_dirs(config: dict):\n",
    "    os.makedirs(config[\"out_root\"], exist_ok=True)\n",
    "    os.makedirs(config[\"index_db\"].parent, exist_ok=True)\n",
    "    os.makedirs(config[\"manifest_db\"].parent, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d5eaa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input_csv(config: dict) -> pl.DataFrame:\n",
    "    return pl.read_csv(source=config[\"csv\"], **config[\"read_csv_opts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac3ff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_composite_keys(df: pl.DataFrame, config: dict) -> pl.DataFrame:\n",
    "    return df.with_columns([\n",
    "        (pl.col(config[\"state_col\"]).cast(str) + \"-\" + pl.col(config[\"county_col\"]).cast(str)).alias(\"locid\"),\n",
    "        (pl.lit(str(config[\"cenyear\"])) + \"-\" + pl.col(config[\"serial_col\"]).cast(str)).alias(\"hhid\"),\n",
    "        (pl.lit(str(config[\"cenyear\"])) + \"-\" + pl.col(config[\"serial_col\"]).cast(str) + \"-\" + pl.col(config[\"histid_col\"]).cast(str)).alias(\"pid\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d752b14",
   "metadata": {},
   "source": [
    "Hash Bucket Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fd61268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_bucket(hhid: str, bucket_count: int) -> int:\n",
    "    h = hashlib.sha256(hhid.encode()).digest()\n",
    "    return int.from_bytes(h[:4], 'big') % bucket_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38bf19bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_buckets(df: pl.DataFrame, config: dict) -> pl.DataFrame:\n",
    "    return df.with_columns([\n",
    "        pl.col(\"hhid\").map_elements(lambda h: hash_bucket(h, config[\"bucket_count\"])).alias(\"bucket\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41ff1c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_buckets_and_collect_manifest(df: pl.DataFrame, config: dict) -> list[dict]:\n",
    "    manifest_rows = []\n",
    "    for b in range(config[\"bucket_count\"]):\n",
    "        bucket_df = df.filter(pl.col(\"bucket\") == b).drop(\"bucket\")\n",
    "        bucket_path = config[\"out_root\"] / f\"bucket_{b:02}.parquet\"\n",
    "        bucket_df.write_parquet(bucket_path, compression=\"zstd\")\n",
    "\n",
    "        manifest_rows.append({\n",
    "            \"cenyear\": config[\"cenyear\"],\n",
    "            \"bucket\": b,\n",
    "            \"file\": str(bucket_path),\n",
    "            \"record_count\": bucket_df.shape[0],\n",
    "            \"min_hhid\": bucket_df[\"hhid\"].min(),\n",
    "            \"max_hhid\": bucket_df[\"hhid\"].max(),\n",
    "            \"min_locid\": bucket_df[\"locid\"].min(),\n",
    "            \"max_locid\": bucket_df[\"locid\"].max()\n",
    "        })\n",
    "    return manifest_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87f88974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_manifest(manifest_rows: list[dict], config: dict):\n",
    "    with duckdb.connect(str(config[\"manifest_db\"])) as con:\n",
    "        con.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS manifest (\n",
    "                cenyear INTEGER,\n",
    "                bucket INTEGER,\n",
    "                file TEXT,\n",
    "                record_count INTEGER,\n",
    "                min_hhid TEXT,\n",
    "                max_hhid TEXT,\n",
    "                min_locid TEXT,\n",
    "                max_locid TEXT,\n",
    "                PRIMARY KEY (cenyear, bucket)\n",
    "            );\n",
    "        \"\"\")\n",
    "        con.register(\"manifest_rows\", pl.DataFrame(manifest_rows))\n",
    "        con.execute(\"INSERT INTO manifest SELECT * FROM manifest_rows;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaafe1a5",
   "metadata": {},
   "source": [
    "Construct Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8aa22c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_search_index(df: pl.DataFrame, config: dict, cenyear: int):\n",
    "    index_df = df.filter(pl.col(config[\"hik_col\"]).is_not_null()).select([\n",
    "        config[\"hik_col\"], \"hhid\", \"locid\"\n",
    "    ]).rename({config[\"hik_col\"]: \"hik\"})\n",
    "\n",
    "    with duckdb.connect(str(config[\"index_db\"])) as con:\n",
    "        con.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS index (\n",
    "                hik   TEXT PRIMARY KEY,\n",
    "                hhid  TEXT,\n",
    "                locid TEXT\n",
    "            );\n",
    "        \"\"\")\n",
    "        con.register(\"index_df\", index_df)\n",
    "        dupes = con.execute(\"SELECT COUNT(*) FROM index WHERE hik IN (SELECT hik FROM index_df);\").fetchone()[0]\n",
    "        if dupes > 0:\n",
    "            raise ValueError(f\"{dupes} duplicate hik(s) detected. Aborting ingestion.\")\n",
    "        con.execute(\"INSERT INTO index SELECT * FROM index_df;\")\n",
    "        print(f\"Cenyear {cenyear}: Appended {index_df.shape[0]} rows to search index.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311cf243",
   "metadata": {},
   "source": [
    "Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9478d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_file(filename: str, folder_map: dict[str,Path]):\n",
    "    config = config_for_filename(filename,folder_map)  # assumes this replaces config_for_year\n",
    "    prepare_output_dirs(config)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    df = read_input_csv(config)\n",
    "    df = add_composite_keys(df, config)\n",
    "    df = assign_buckets(df, config)\n",
    "\n",
    "    manifest_rows = write_buckets_and_collect_manifest(df, config)\n",
    "    write_manifest(manifest_rows, config)\n",
    "    build_search_index(df, config, config[\"cenyear\"])\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    elapsed = round(end - start, 2)\n",
    "    print(f\"Finished processing {filename} in {elapsed} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db684163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1dffa2d5514d6d970ebf4bd1c6fbd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cenyear 1850: Appended 8854602 rows to search index.\n",
      "Finished processing cs1850.csv.gz in 60.04 seconds.\n"
     ]
    }
   ],
   "source": [
    "filelist = [\"cs1850.csv.gz\"]\n",
    "for filename in filelist:\n",
    "    ingest_file(filename, folder_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (family_migration)",
   "language": "python",
   "name": "family_migration"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
